{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "> \"How to use Pytorch quantization API for model quantization\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: Atmadeep Banerjee\n",
    "- use_math: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What?\n",
    "\n",
    "So you have trained a neural network and want to deploy it. Performance — speed and computational complexity, *not* just accuracy — matters a *lot* when in production. If your model can achieve low enough latencies on a cpu instance, you will have a massively lower deployment cost over using a gpu instance. Lower costs equals higher profits.\n",
    "\n",
    "Model quantization is (usually) the easiest way to massively speed up your model. If you want to learn more about the theory behind quantization and how it works check out this [blogpost](https://sharechat.com/blogs/data-science/neural-network-compression-using-quantization). Feeling too lazy to read through all that? Here’s a quick summary. Quantization provides us a way to compress the weights of our model. Weights are usually represented with 32-bit floats. But we \"quantize\" the weights and reduce this to 8-bits instead. You can go even further and use as less as 1-bit for every parameter, creating binary neural networks, but that is beyond the scope of this post. While quantization directly reduces model size by 4x, that is not the most important part. Using reduced precisions *significantly* reduces the time taken for matrix multiplication and addition. These are not measly 10-20% gains. You can expect a 3-5x speed up when quantizing a model from FP32 to INT8. These gains are serious enough that they offset the performance gap between a CPU and GPU, making real time inference possible on CPU.\n",
    "\n",
    "So… what’s the catch you ask? The catch is that using lower precision arithmetic means there is an increased chance of arithmetic overflow — because we are greatly limiting the range in which values can lie. There are ways to reduce the probability of overflow (more on this later) but the chances still remain.\n",
    "\n",
    "<!-- Knowledge distillation is also a cool thing to try as well but unlike quantization, it might need you to make non-trivial changes to your training loop. Especially if you are doing things more complex than standard classification. Depending on your task you might also need a lot of extra experimentation to get knowledge distillation to work well. But if you can get it to work, you can seriously reduce your model size with a minor hit in performance. Combine that with quantization and you will have blazing fast inference. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How?\n",
    "\n",
    "Quantizing common pytorch models are pretty simple thanks to Pytorch's quantization API. You need to perform the following steps to get a basic quantized model\n",
    "\n",
    "#### Step 0: Create a model\n",
    "\n",
    "Let's create a basic resnet18 model with a binary classification head. Note that we need to use the 'quantization' version of resnet18, instead of standard torchvision version. The latter will give an error. I will explain the reason for this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = nn.Sequential(\n",
    "    *list(models.quantization.resnet18(pretrained=True).children())[:-3], \n",
    "    nn.Flatten(), \n",
    "    nn.Linear(512,2)\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Fuse layers\n",
    "\n",
    "In this step we will 'combine' the layers of our model. This step is actually not related to quantization, but it does give extra speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.quantization.fuse_modules(resnet, [['0', '1', '2']], inplace=True)\n",
    "for i in range(4,8):\n",
    "    torch.quantization.fuse_modules(resnet[i][0], \n",
    "                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n",
    "                                    inplace=True)\n",
    "    if resnet[i][0].downsample is not None:\n",
    "        torch.quantization.fuse_modules(resnet[i][0].downsample, \n",
    "                                        [['0', '1']], \n",
    "                                        inplace=True)\n",
    "    torch.quantization.fuse_modules(resnet[i][1], \n",
    "                                    [['conv1', 'bn1', 'relu'], ['conv2', 'bn2']], \n",
    "                                    inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  Prepare for qat\n",
    "\n",
    "Prepare the model for quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "resnet = torch.quantization.prepare_qat(resnet).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train normally\n",
    "\n",
    "#### Step 4: Post training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qresnet(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.q = torch.quantization.QuantStub()\n",
    "        self.m = m\n",
    "        self.dq = torch.quantization.DeQuantStub()\n",
    "    def forward(self, x):\n",
    "        return self.dq(self.m(self.q(x)))\n",
    "\n",
    "# load the best model from training phase\n",
    "resnet.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# wrap qat resnet with quant dequant stubs\n",
    "qmodel = Qresnet(resnet)\n",
    "\n",
    "# add quantization recipe to model\n",
    "qmodel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# prepare the modules in the model to be quantized\n",
    "qmodel = torch.quantization.prepare(qmodel)\n",
    "\n",
    "# calibrate weights\n",
    "for x,y in train_loader:\n",
    "    qmodel(x.cuda())\n",
    "\n",
    "# actually quantize the trained model. \n",
    "qmodel = torch.quantization.convert(qmodel.cpu())\n",
    "\n",
    "# put to eval mode\n",
    "qmodel = qmodel.eval()\n",
    "\n",
    "# script the model using TorchScript for easy delpoyment\n",
    "torch.jit.script(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
